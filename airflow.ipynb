{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is scalable, dynamic, extensible, and lean \n",
    "\n",
    "The five main features of Apache Airflow are pure Python, useful UI, integration, easy to use, and open-source \n",
    "\n",
    "A common use case is that Apache Airflow defines and organizes machine learning pipeline dependencies \n",
    "\n",
    "Tasks are created with Airflow operators \n",
    "\n",
    "Pipelines are specified as dependencies between tasks \n",
    "\n",
    "Pipeline DAGs defined as code are more maintainable, testable, and collaborative \n",
    "\n",
    "Apache Airflow has a rich UI that simplifies working with data pipelines \n",
    "\n",
    "You can visualize your DAG in graph or grid mode \n",
    "\n",
    "Key components of a DAG definition file include DAG arguments, DAG and task definitions, and the task pipeline \n",
    "\n",
    "Set a schedule to specify how often to re-run your DAG\n",
    "\n",
    "You can save Airflow logs into local file systems and send them to cloud storage, search engines, and log analyzers \n",
    "\n",
    "Airflow recommends sending production deployment logs to be analyzed by Elasticsearch or Splunk \n",
    "\n",
    "You can view DAGs and task events with Airflow’s UI\n",
    "\n",
    "The three types of Airflow metrics are counters, gauges, and timers \n",
    "\n",
    "Airflow recommends that production deployment metrics be sent to and analyzed by Prometheus via StatsD\n",
    "\n",
    "**Batch loading** refers to loading data in chunks defined by some time windows of data accumulated by the data source, usually on the order of hours to days.\n",
    "\n",
    "An ETL job can be scheduled to run by creating **a cron job** for your Bash script.\n",
    "\n",
    "ETL is curating data from multiple sources, conforming it to a unified data format or structure, and loading the transformed data to its new environment.\n",
    "\n",
    "You can use BashOperator to invoke bash commands or scripts.\n",
    "\n",
    "Counters are metrics that will always be increasing, such as the total counts of successful or failed tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow dags list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow dags list|grep \"my-first-python-etl-dag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow tasks list my-first-python-etl-dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow tasks list example_bash_operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow tasks list example_branch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow dags unpause tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "airflow dags pause tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy DAG file as my_dag.py to __airflow/dags__ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    \"simple_dag\",\n",
    "    default_args=default_args,\n",
    "    description=\"A simple DAG\",\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    start_date=datetime(2023, 7, 1),\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "t1 = BashOperator(\n",
    "    task_id=\"print_date\",\n",
    "    bash_command=\"date\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id=\"print_hello\",\n",
    "    bash_command='echo \"Hello World!\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Set task dependencies\n",
    "t1 >> t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Define default arguments for the DAG\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2024, 7, 17),\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    \"simple_example_dag\",\n",
    "    default_args=default_args,\n",
    "    description=\"A simple example DAG\",\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "\n",
    "# Define tasks\n",
    "def task_1():\n",
    "    print(\"Executing Task 1\")\n",
    "\n",
    "\n",
    "def task_2():\n",
    "    print(\"Executing Task 2\")\n",
    "\n",
    "\n",
    "def task_3():\n",
    "    print(\"Executing Task 3\")\n",
    "\n",
    "\n",
    "# Create task instances\n",
    "t1 = PythonOperator(\n",
    "    task_id=\"task_1\",\n",
    "    python_callable=task_1,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "t2 = PythonOperator(\n",
    "    task_id=\"task_2\",\n",
    "    python_callable=task_2,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "t3 = PythonOperator(\n",
    "    task_id=\"task_3\",\n",
    "    python_callable=task_3,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Set task dependencies\n",
    "t1 >> [t2, t3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample DAG from Coursera\n",
    "\n",
    "a DAG file, my_first_dag.py, which will run daily. Defines tasks execute_extract, execute_transform, execute_load, and execute_check to call the respective Python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from datetime import timedelta\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Operators; you need this to write tasks!\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "# Define the path for the input and output files\n",
    "input_file = \"/etc/passwd\"\n",
    "extracted_file = \"extracted-data.txt\"\n",
    "transformed_file = \"transformed.txt\"\n",
    "output_file = \"data_for_analytics.csv\"\n",
    "\n",
    "\n",
    "def extract():\n",
    "    global input_file\n",
    "    print(\"Inside Extract\")\n",
    "    # Read the contents of the file into a string\n",
    "    with open(input_file, \"r\") as infile, open(extracted_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            fields = line.split(\":\")\n",
    "            if len(fields) >= 6:\n",
    "                field_1 = fields[0]\n",
    "                field_3 = fields[2]\n",
    "                field_6 = fields[5]\n",
    "                outfile.write(field_1 + \":\" + field_3 + \":\" + field_6 + \"\\n\")\n",
    "\n",
    "\n",
    "def transform():\n",
    "    global extracted_file, transformed_file\n",
    "    print(\"Inside Transform\")\n",
    "    with open(extracted_file, \"r\") as infile, open(transformed_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            processed_line = line.replace(\":\", \",\")\n",
    "            outfile.write(processed_line + \"\\n\")\n",
    "\n",
    "\n",
    "def load():\n",
    "    global transformed_file, output_file\n",
    "    print(\"Inside Load\")\n",
    "    # Save the array to a CSV file\n",
    "    with open(transformed_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def check():\n",
    "    global output_file\n",
    "    print(\"Inside Check\")\n",
    "    # Save the array to a CSV file\n",
    "    with open(output_file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    \"owner\": \"Your name\",\n",
    "    \"start_date\": days_ago(0),\n",
    "    \"email\": [\"your email\"],\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    \"my-first-python-etl-dag\",\n",
    "    default_args=default_args,\n",
    "    description=\"My first DAG\",\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Define the task named execute_extract to call the `extract` function\n",
    "execute_extract = PythonOperator(\n",
    "    task_id=\"extract\",\n",
    "    python_callable=extract,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_transform to call the `transform` function\n",
    "execute_transform = PythonOperator(\n",
    "    task_id=\"transform\",\n",
    "    python_callable=transform,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_load to call the `load` function\n",
    "execute_load = PythonOperator(\n",
    "    task_id=\"load\",\n",
    "    python_callable=load,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_load to call the `load` function\n",
    "execute_check = PythonOperator(\n",
    "    task_id=\"check\",\n",
    "    python_callable=check,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task pipeline\n",
    "execute_extract >> execute_transform >> execute_load >> execute_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG: ETL Server Access Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from datetime import timedelta\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Operators; you need this to write tasks!\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "import requests\n",
    "\n",
    "# Define the path for the input and output files\n",
    "input_file = \"web-server-access-log.txt\"\n",
    "extracted_file = \"extracted-data.txt\"\n",
    "transformed_file = \"transformed.txt\"\n",
    "output_file = \"capitalized.txt\"\n",
    "\n",
    "\n",
    "def download_file():\n",
    "    url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\"\n",
    "    # Send a GET request to the URL\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        # Raise an exception for HTTP errors\n",
    "        response.raise_for_status()\n",
    "        # Open a local file in binary write mode\n",
    "        with open(input_file, \"wb\") as file:\n",
    "            # Write the content to the local file in chunks\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "    print(f\"File downloaded successfully: {input_file}\")\n",
    "\n",
    "\n",
    "def extract():\n",
    "    global input_file\n",
    "    print(\"Inside Extract\")\n",
    "    # Read the contents of the file into a string\n",
    "    with open(input_file, \"r\") as infile, open(extracted_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            fields = line.split(\"#\")\n",
    "            if len(fields) >= 4:\n",
    "                field_1 = fields[0]\n",
    "                field_4 = fields[3]\n",
    "                outfile.write(field_1 + \"#\" + field_4 + \"\\n\")\n",
    "\n",
    "\n",
    "def transform():\n",
    "    global extracted_file, transformed_file\n",
    "    print(\"Inside Transform\")\n",
    "    with open(extracted_file, \"r\") as infile, open(transformed_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            processed_line = line.upper()\n",
    "            outfile.write(processed_line + \"\\n\")\n",
    "\n",
    "\n",
    "def load():\n",
    "    global transformed_file, output_file\n",
    "    print(\"Inside Load\")\n",
    "    # Save the array to a CSV file\n",
    "    with open(transformed_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def check():\n",
    "    global output_file\n",
    "    print(\"Inside Check\")\n",
    "    # Save the array to a CSV file\n",
    "    with open(output_file, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    \"owner\": \"Your name\",\n",
    "    \"start_date\": days_ago(0),\n",
    "    \"email\": [\"your email\"],\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    \"my-first-python-etl-dag\",\n",
    "    default_args=default_args,\n",
    "    description=\"My first DAG\",\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Define the task named download to call the `download_file` function\n",
    "download = PythonOperator(\n",
    "    task_id=\"download\",\n",
    "    python_callable=download_file,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_extract to call the `extract` function\n",
    "execute_extract = PythonOperator(\n",
    "    task_id=\"extract\",\n",
    "    python_callable=extract,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_transform to call the `transform` function\n",
    "execute_transform = PythonOperator(\n",
    "    task_id=\"transform\",\n",
    "    python_callable=transform,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_load to call the `load` function\n",
    "execute_load = PythonOperator(\n",
    "    task_id=\"load\",\n",
    "    python_callable=load,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define the task named execute_load to call the `load` function\n",
    "execute_check = PythonOperator(\n",
    "    task_id=\"check\",\n",
    "    python_callable=check,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task pipeline\n",
    "download >> execute_extract >> execute_transform >> execute_load >> execute_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Auto-refresh in Airflow WEB UI, after adding the .py file under /dags folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "# defining DAG arguments\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    \"owner\": \"Your name\",\n",
    "    \"start_date\": days_ago(0),\n",
    "    \"email\": [\"your email\"],\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# defining the DAG\n",
    "dag = DAG(\n",
    "    \"dummy_dag\",\n",
    "    default_args=default_args,\n",
    "    description=\"My first DAG\",\n",
    "    schedule_interval=timedelta(minutes=1),\n",
    ")\n",
    "\n",
    "# define the tasks\n",
    "\n",
    "# define the first task\n",
    "\n",
    "task1 = BashOperator(\n",
    "    task_id=\"task1\",\n",
    "    bash_command=\"sleep 1\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# define the second task\n",
    "task2 = BashOperator(\n",
    "    task_id=\"task2\",\n",
    "    bash_command=\"sleep 2\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# define the third task\n",
    "task3 = BashOperator(\n",
    "    task_id=\"task3\",\n",
    "    bash_command=\"sleep 3\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# task pipeline\n",
    "task1 >> task2 >> task3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
